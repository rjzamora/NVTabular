{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time \n",
    "# from ds_itr.adamw import AdamW as AW\n",
    "\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.tabular import *\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.tabular import TabularModel\n",
    "\n",
    "import cudf\n",
    "\n",
    "from nv_tabular.preproc import Workflow\n",
    "from nv_tabular.ops import Normalize, FillMissing, Categorify, Moments, Median, Encoder, LogOp, ZeroFill\n",
    "from nv_tabular.dl_encoder import DLLabelEncoder\n",
    "from nv_tabular.ds_iterator import GPUDatasetIterator\n",
    "from nv_tabular.batchloader import FileItrDataset, DLCollator, DLDataLoader, TensorItrDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__, cudf.__version__, fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz\n",
    "# load snakeviz if you want to run profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_cpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datafolder = '/datasets/rossmann/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf = pd.read_csv(f'{datafolder}/train.csv', low_memory=False)\n",
    "# testdf = pd.read_csv(f'{datafolder}/test.csv', low_memory=False)\n",
    "# storedf = pd.read_csv(f'{datafolder}/store.csv', low_memory=False)\n",
    "# print(\"Preprocessing...\")\n",
    "\n",
    "categorical_cols = [\n",
    "    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'Week', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "    'StoreType', 'Assortment', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "    'Promo2SinceYear', 'PromoInterval',\n",
    "]\n",
    "continuous_cols = [\n",
    "    'CompetitionDistance'\n",
    "]\n",
    "# traindf = traindf.drop(['Customers', 'Open'], axis=1)\n",
    "# testdf = testdf.drop(['Open', 'Id'], axis=1)\n",
    "\n",
    "# def modify_df(df):\n",
    "#     initlen = len(df)\n",
    "\n",
    "#     # Expand date into [Year, Month, Day, Week]\n",
    "#     df.Date = pd.to_datetime(df.Date)\n",
    "#     for attr in ['Year', 'Month', 'Day', 'Week']:\n",
    "#         df[attr] = getattr(pd.DatetimeIndex(df.Date), attr.lower())\n",
    "\n",
    "#     # Join store.csv and train/test.csv on Store\n",
    "#     df = pd.merge(df, storedf, on='Store')\n",
    "\n",
    "#     # Fix NaNs in *Since* columns\n",
    "#     defaults = {'CompetitionOpenSinceYear': 1900, 'CompetitionOpenSinceMonth': 1,\n",
    "#                 'Promo2SinceYear': 1900, 'Promo2SinceWeek': 1, 'CompetitionDistance': 0}\n",
    "#     df = df.fillna(defaults)\n",
    "\n",
    "#     # Ensure no rows lost on the join\n",
    "#     assert len(df) == initlen\n",
    "\n",
    "#     # Cast continuous columns to float\n",
    "#     df = df.astype({x: 'float32' for x in continuous_cols})\n",
    "\n",
    "#     # StateHoliday is a special case\n",
    "#     df.StateHoliday = df.StateHoliday.astype(str).replace(\"0\", \"d\")\n",
    "\n",
    "#     # Move the continuous columns to the end\n",
    "#     for column in continuous_cols:\n",
    "#         cd = df.pop(column)\n",
    "#         df[column] = cd\n",
    "#     return df\n",
    "\n",
    "# traindf = modify_df(traindf)\n",
    "# testdf = modify_df(testdf)\n",
    "\n",
    "# # Filter out 0 sales from train\n",
    "# traindf = traindf[traindf.Sales > 0]\n",
    "\n",
    "# # Index categorical columns across both train and test.\n",
    "# for col in categorical_cols:\n",
    "#     # Get the default type for the column\n",
    "#     default_type = traindf[col].dtype.type() if traindf[col].dtype.type() is not None else ''\n",
    "#     # Fix NaNs with the default value for the column's type.\n",
    "#     traindf[col] = traindf[col].fillna(default_type)\n",
    "#     testdf[col] = testdf[col].fillna(default_type)\n",
    "\n",
    "# # Move the sales column to the end\n",
    "# sales = traindf.pop('Sales')\n",
    "# traindf['Sales'] = sales\n",
    "\n",
    "# # Make a validation set from the equivalent period of the test set in the training set, a year before.\n",
    "# t0 = testdf.Date.min() - datetime.timedelta(365)\n",
    "# t1 = testdf.Date.max() - datetime.timedelta(365)\n",
    "# val_mask = (traindf.Date > t0) & (traindf.Date <= t1)\n",
    "# valdf, traindf = traindf[val_mask], traindf[~val_mask]\n",
    "\n",
    "# # Drop unnecessary columns\n",
    "# traindf = traindf.drop('Date', axis=1)\n",
    "# valdf = valdf.drop('Date', axis=1)\n",
    "# testdf = testdf.drop('Date', axis=1)\n",
    "\n",
    "# print(\"Saving to file...\")\n",
    "# traindf.to_csv(f'{datafolder}/train_fin.csv', index=False)\n",
    "# testdf.to_csv(f'{datafolder}/test_fin.csv', index=False)\n",
    "# valdf.to_csv(f'{datafolder}/val_fin.csv', index=False)\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset Gathering: Define files in the training and validation datasets. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = os.path.join(datafolder , 'train_fin.csv')\n",
    "valid_set = [datafolder + 'val_fin.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = cudf.read_csv(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday', 'Year',\n",
    "       'Month', 'Day', 'Week', 'StoreType', 'Assortment',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval',\n",
    "       'CompetitionDistance', 'Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Grab column information</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = [\n",
    "        'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'Week', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "        'StoreType', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Assortment',\n",
    "        'Promo2SinceYear', 'PromoInterval',\n",
    "    ]\n",
    "cont_names = [\n",
    "        'CompetitionDistance'\n",
    "    ] \n",
    "cat_names = [name for name in cat_names if name in cols]\n",
    "cont_names = [name for name in cont_names if name in cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing:</h3> <p>Select operations to perform, create the Preprocessor object, create dataset iterator object and collect the stats on the training dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc = Workflow(cat_names=cat_names, cont_names=cont_names, label_name=['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_feature(FillMissing())\n",
    "proc.add_preprocess(Normalize())\n",
    "proc.add_preprocess(Categorify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trains_itrs = GPUDatasetIterator(train_set,names=cols, engine='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc.update_stats(trains_itrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.create_final_cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Gather embeddings using statistics gathered in the Read phase.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [x[1] for x in proc.df_ops['Categorify'].get_emb_sz(proc.stats[\"categories\"], proc.columns_ctx[\"categorical\"]['base'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(proc.columns_ctx[\"categorical\"]['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valids_itr = GPUDatasetIterator(valid_set,names=cols, engine='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Create the file iterators using the FileItrDataset Class.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xy = proc.ds_to_tensors(trains_itrs)\n",
    "val_xy = proc.ds_to_tensors(valids_itr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_sets = [train_xy[0], train_xy[1], train_xy[2]]\n",
    "v_sets = [val_xy[0], val_xy[1], val_xy[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_batch_sets = [TensorItrDataset(t_sets, batch_size=1024) for i in range(10)]\n",
    "v_batch_sets = [TensorItrDataset(v_sets, batch_size=1024) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_chain = torch.utils.data.ChainDataset(t_batch_sets)\n",
    "v_chain = torch.utils.data.ChainDataset(v_batch_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Use the Deep Learning Collator to create a collate function to pass to the dataloader.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_col(batch):\n",
    "    batch = batch[0]\n",
    "    return batch[0], batch[1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_data = DLDataLoader(t_chain, collate_fn=gen_col, pin_memory=False, num_workers=0)\n",
    "v_data = DLDataLoader(v_chain, collate_fn=gen_col, pin_memory=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>After creating the Dataloaders you can leverage fastai framework to create Machine Learning models</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = DataBunch(t_data, v_data, collate_fn=gen_col, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = TabularModel(emb_szs = embeddings, n_cont=len(cont_names), out_sz=1, layers=[1000,500])\n",
    "\n",
    "learn =  Learner(databunch, model)\n",
    "learn.loss_func = MSELossFlat()\n",
    "# learn.opt_func = AW\n",
    "learn.lr_find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(show_moms=True, suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.75-2\n",
    "epochs = 10\n",
    "from fastai.utils.mod_display import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "learn.fit(epochs,learning_rate)\n",
    "t_final = time() - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn \n",
    "del model\n",
    "del databunch\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
