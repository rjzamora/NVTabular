{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time \n",
    "\n",
    "from fastai import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.tabular import *\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.tabular import TabularModel\n",
    "\n",
    "import cudf\n",
    "import nv_tabular as nvt\n",
    "from nv_tabular.ops import Normalize, FillMissing, Categorify, Moments, Median, Encoder, LogOp, ZeroFill\n",
    "from nv_tabular.batchloader import FileItrDataset, DLCollator, DLDataLoader\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext snakeviz\n",
    "# load snakeviz if you want to run profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_cpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3> Dataset Gathering: Define files in the training and validation datasets. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # data_path = '/rapids/notebooks/jperez/Documents/ds-itr/examples/'\n",
    "# data_path = '/datasets/criteo/criteo_embed/'\n",
    "# #df_test = 'test/'\n",
    "# df_valid = ''\n",
    "# df_train = ''\n",
    "# split = 332\n",
    "# end = 332\n",
    "#train_set = [data_path + df_train + x for x in os.listdir(data_path + df_train) if  x.endswith('parquet') and int(x.split(\".\")[0]) in train_days] \n",
    "#valid_set = [data_path + df_valid + x for x in os.listdir(data_path + df_valid) if  x.endswith('parquet') and int(x.split(\".\")[0]) in valid_days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/rapids/notebooks/jperez/Documents/ds-itr/examples/'\n",
    "# data_path = '/datasets/criteo/raw_csvs/split_train_data_parquet/'\n",
    "data_path = '/home/oyilmaz/Documents/recsys_data/criteo_embed/'\n",
    "#df_test = 'test/'\n",
    "df_valid = ''\n",
    "df_train = ''\n",
    "start = 0\n",
    "split = 3\n",
    "fin = 5\n",
    "\n",
    "train_days = [\"day_\" + str(x) for x in range(start, split)]\n",
    "valid_days = [\"day_\" + str(x) for x in range(split, fin)]\n",
    "print(train_days, valid_days)\n",
    "\n",
    "train_set = [data_path + df_train + x for x in os.listdir(data_path + df_train) if x.endswith(\"parquet\")][start:split]\n",
    "valid_set = [data_path + df_train + x for x in os.listdir(data_path + df_train) if x.endswith(\"parquet\")][split:fin]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set), len(valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Grab column information</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_names = ['I' + str(x) for x in range(1,14)]\n",
    "cat_names =  ['C' + str(x) for x in range(1,24)]\n",
    "cat_names, cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['label']  + cont_names + cat_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing:</h3> <p>Select operations to perform, create the Preprocessor object, create dataset iterator object and collect the stats on the training dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc = nvt.Workflow(cat_names=cat_names, cont_names=cont_names, label_name=['label'], to_cpu=to_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc.add_cont_feature([ZeroFill(replace=True), LogOp(replace=True)])\n",
    "proc.add_cont_preprocess(Normalize(replace=True))\n",
    "proc.add_cat_preprocess(Categorify(replace=True, use_frequency=True, freq_threshold=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trains_itrs = nvt.dataset(train_set, names=cols, engine='parquet', gpu_memory_frac=0.1)\n",
    "valids_itrs = nvt.dataset(valid_set, names=cols, engine='parquet', gpu_memory_frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train = '/home/oyilmaz/Documents/recsys_data/outputs/'\n",
    "output_valid = '/home/oyilmaz/Documents/recsys_data/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "proc.apply(trains_itrs, apply_offline=True, record_stats=True, shuffle=True, output_path=output_train, num_out_files=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#proc.apply(valids_itrs, apply_offline=True, record_stats=False, shuffle=True, output_path=output_valid, num_out_files=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_set = [os.path.join(output_train, x) for x in os.listdir(output_train) if x.endswith(\"parquet\")]\n",
    "new_valid_set = [os.path.join(output_valid, x) for x in os.listdir(output_valid) if x.endswith(\"parquet\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Gather embeddings using statistics gathered in the Read phase.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [x[1] for x in proc.df_ops['Categorify'].get_emb_sz(proc.stats[\"categories\"], proc.columns_ctx['categorical']['base'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Create the file iterators using the FileItrDataset Class.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_batch_sets = [FileItrDataset(x, names=cols, engine='parquet', batch_size=1000, sep=\"\\t\") for x in train_set]\n",
    "v_batch_sets = [FileItrDataset(x, names=cols, engine='parquet', batch_size=10000, sep=\"\\t\") for x in valid_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_chain = torch.utils.data.ChainDataset(t_batch_sets)\n",
    "v_chain = torch.utils.data.ChainDataset(v_batch_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Use the Deep Learning Collator to create a collate function to pass to the dataloader.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dlc = DLCollator(preproc=proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_data = DLDataLoader(t_chain, collate_fn=dlc.gdf_col, pin_memory=False, num_workers=0)\n",
    "v_data = DLDataLoader(v_chain, collate_fn=dlc.gdf_col, pin_memory=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>After creating the Dataloaders you can leverage fastai framework to create Machine Learning models</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = DataBunch(t_data, v_data, collate_fn=dlc.gdf_col, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = TabularModel(emb_szs = embeddings, n_cont=len(cont_names), out_sz=2, layers=[512,256])\n",
    "\n",
    "learn =  Learner(databunch, model, metrics=[accuracy])\n",
    "learn.loss_func = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(show_moms=True, suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.32e-2\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "learn.fit_one_cycle(epochs,learning_rate)\n",
    "t_final = time() - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn \n",
    "del model\n",
    "del databunch\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
