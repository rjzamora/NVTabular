{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_id = 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cudf\n",
    "import pandas as pd\n",
    "import time\n",
    "import nvstrings\n",
    "import rmm\n",
    "import ds_itr.ds_iterator as di\n",
    "import sys\n",
    "try:\n",
    "    import cupy as cp\n",
    "except ImportError:\n",
    "    import numpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0a+1941.g3efd22b.dirty'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cudf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nfs/rzamora/workspace/rapids-dl/ds-itr/examples\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_gpu(words,func,arg=None,dtype=cp.int32):\n",
    "    res = rmm.device_array(words.size(), dtype=dtype)\n",
    "    if arg is None:\n",
    "        cmd = 'words.%s(res.device_ctypes_pointer.value)'%(func)\n",
    "    else:\n",
    "        cmd = 'words.%s(arg,res.device_ctypes_pointer.value)'%(func)\n",
    "    eval(cmd)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_split(gdf, num_splits=5, save_path=''):\n",
    "    if len(gdf) < num_splits:\n",
    "        return\n",
    "    # grab all files in save_path\n",
    "    # get batch size\n",
    "    gdf = gdf.reindex()\n",
    "    batch = len(gdf) // num_splits\n",
    "    for i in range(0, len(gdf), batch):\n",
    "        # append batch to file set\n",
    "        gdf[i:i+batch].to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(gdf1, gdf2_location, on, how='inner', file=False, gpu_memory_frac=0.5, gdf1_pri=False, clean_up=True):\n",
    "    itrs = []\n",
    "    if file:\n",
    "        r_fi  = [gdf2_location]\n",
    "    else:\n",
    "        r_fi = [gdf2_location + x for x in os.listdir(gdf2_location) if x.endswith(\".parquet\")]\n",
    "    itr = di.GPUDatasetIterator(r_fi, gpu_memory_frac=gpu_memory_frac, use_row_groups=True)\n",
    "    new_data_gd = cudf.DataFrame()\n",
    "    for item in itr:\n",
    "        data_temp_gd = item.merge(gdf1, on=on, how=how) if not gdf1_pri else gdf1.merge(item, on=on, how=how)\n",
    "        new_data_gd = cudf.concat([new_data_gd, data_temp_gd], axis=0) if new_data_gd else data_temp_gd\n",
    "        del item, data_temp_gd\n",
    "    del itr\n",
    "    del gdf1\n",
    "    if clean_up:\n",
    "        for fi in r_fi:\n",
    "            os.remove(fi)\n",
    "    return new_data_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('cache')==False:\n",
    "    os.mkdir('cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/datasets/trivago/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cudf read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15932992, 12) (3782335, 12)\n",
      "CPU times: user 3.06 s, sys: 1.96 s, total: 5.02 s\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train  = cudf.read_csv('%s/train.csv'%path)\n",
    "test = cudf.read_csv('%s/test.csv'%path)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "data_gd = cudf.concat([train, test])\n",
    "del train, test\n",
    "submission_gd = cudf.read_csv('%s/submission_popular.csv'%path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of clickouts: 2115365\n",
      "true test (253573, 13)\n",
      "true test shape match submission shape\n",
      "CPU times: user 1.87 s, sys: 1.58 s, total: 3.45 s\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_gd['is_click_out'] = on_gpu(data_gd['action_type'].data,'compare',arg='clickout item')\n",
    "data_gd['is_click_out'] = data_gd['is_click_out']==0 # 0 means string match\n",
    "data_gd['is_click_out'] = data_gd['is_click_out'].astype('bool')\n",
    "data_gd = data_gd[data_gd['is_click_out']]\n",
    "\n",
    "data_gd.drop_column('is_click_out')\n",
    "print(\"# of clickouts:\",data_gd.shape[0])\n",
    "data_gd['clickout_missing'] = data_gd['reference'].isnull()\n",
    "\n",
    "print('true test',data_gd[data_gd['clickout_missing']].shape)\n",
    "assert submission_gd.shape[0] == data_gd[data_gd['clickout_missing']].shape[0]\n",
    "print('true test shape match submission shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 8 ms, total: 128 ms\n",
      "Wall time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_gd['row_id'] = cp.arange(data_gd.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting, over 10GB GPU memory Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.24 s, sys: 688 ms, total: 4.93 s\n",
      "Wall time: 9.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_gd.to_parquet('data_gd/', chunk_size=100000)\n",
    "del data_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 592 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "r_fi = ['data_gd/' + x for x in os.listdir('data_gd/') if x.endswith(\".parquet\")]\n",
    "data_itr = di.GPUDatasetIterator(r_fi, gpu_memory_frac=0.01, use_row_groups=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3672953, 17) (917655, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3664951, 17) (916957, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3669690, 17) (917480, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3663781, 17) (915556, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3670316, 17) (917173, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3665316, 17) (915441, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3669509, 17) (916572, 17) (0, 17)\n",
      "valid\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(3553653, 17) (887824, 17) (147900, 17)\n",
      "valid\n",
      "test\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(1917527, 17) (481415, 17) (2194933, 17)\n",
      "valid\n",
      "test\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(1931980, 17) (482200, 17) (2176238, 17)\n",
      "valid\n",
      "test\n",
      "train\n",
      "items export\n",
      "merging with items...\n",
      "merging with items binary meta...\n",
      "done with concatenation\n",
      "splitting\n",
      "exporting\n",
      "(1125017, 17) (283070, 17) (1243462, 17)\n",
      "valid\n",
      "test\n",
      "train\n",
      "CPU times: user 15min 51s, sys: 1min 39s, total: 17min 31s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_ones = 0\n",
    "for data_gd in data_itr:\n",
    "    data_gd['row_id'] = cp.arange(data_gd.shape[0])\n",
    "    #----------\n",
    "    # pull apart impressions and prices\n",
    "    #----------\n",
    "    candidates_gd = data_gd['impressions'].data.split('|')\n",
    "    prices_gd = data_gd['prices'].data.split('|')\n",
    "    data_gd.drop_column('impressions')\n",
    "    data_gd.drop_column('prices')\n",
    "    data_gd_rec_list = data_gd[['row_id']].to_pandas()\n",
    "    #----------\n",
    "    # map prices to items\n",
    "    #----------\n",
    "    for i in range(len(candidates_gd)):\n",
    "        data_gd_rec_list['item_%d'%i] = candidates_gd[i].to_host()\n",
    "        data_gd_rec_list['price_%d'%i] = prices_gd[i].to_host()\n",
    "    del prices_gd, candidates_gd\n",
    "    data_gd_rec_list = data_gd_rec_list.set_index('row_id')\n",
    "    #----------\n",
    "    # pull item specific information\n",
    "    #----------\n",
    "    cols = [i for i in data_gd_rec_list.columns if i.startswith('item_')]\n",
    "    items = data_gd_rec_list[cols].stack().reset_index()\n",
    "    items.columns = ['row_id','candidate_order','item_id']\n",
    "    #----------\n",
    "    # get pricing info for items\n",
    "    #----------\n",
    "    cols = [i for i in data_gd_rec_list.columns if i.startswith('price_')]\n",
    "    prices = data_gd_rec_list[cols].stack().reset_index()\n",
    "    del data_gd_rec_list\n",
    "    #----------\n",
    "    # combine items and prices dataframes \n",
    "    #----------\n",
    "    prices.columns = ['row_id','candidate_order','price']\n",
    "    items['price'] = prices['price'].astype(int)\n",
    "    del prices\n",
    "    items['candidate_order'] = items['candidate_order'].apply(lambda x:x.split('_')[1]).astype(int)\n",
    "    count = items['row_id'].value_counts()\n",
    "    items['row_id_count'] = items['row_id'].map(count)\n",
    "    items = items[items['row_id_count']>1]\n",
    "    #----------\n",
    "    # export items dataframe to parquet\n",
    "    #----------\n",
    "    print('items export')\n",
    "    items.to_parquet('items.parquet')\n",
    "    del items\n",
    "    data_gd['clickout_missing'] = data_gd['clickout_missing'].astype(int)\n",
    "    print('merging with items...')\n",
    "    #----------\n",
    "    # merge main dataframe chunk with items \n",
    "    #----------\n",
    "    data_gd = merging(data_gd, 'items.parquet', 'row_id', how='left', file=True, gpu_memory_frac=0.10, clean_up=False)\n",
    "    print('merging with items binary meta...')\n",
    "    data_gd['item_id'] = data_gd['item_id'].astype(int)\n",
    "    #----------\n",
    "    # merge main dataframe chunk with items binary feature data\n",
    "    #----------\n",
    "    data_pair_gd = data_gd\n",
    "    print('done with concatenation')\n",
    "#######\n",
    "    data_pair_gd['reference'] = data_pair_gd['reference'].astype(int)\n",
    "    data_pair_gd['item_id'] = data_pair_gd['item_id'].astype(int)\n",
    "    data_pair_gd['target'] = data_pair_gd['reference'] == data_pair_gd['item_id']\n",
    "    data_pair_gd['target'] = data_pair_gd['target'].astype(int)\n",
    "    print('splitting')\n",
    "\n",
    "    train_pair_gd = data_pair_gd[data_pair_gd.clickout_missing==0]\n",
    "    test_pair_gd = data_pair_gd[data_pair_gd.clickout_missing>0]\n",
    "    del data_pair_gd\n",
    "    train_pair_gd['is_va'] = train_pair_gd.row_id%5 == 0\n",
    "    train_pair = train_pair_gd[train_pair_gd.is_va==0]\n",
    "    valid_pair = train_pair_gd[train_pair_gd.is_va>0]\n",
    "    del train_pair_gd\n",
    "    train_pair = train_pair.drop(['is_va'])\n",
    "    valid_pair = valid_pair.drop(['is_va'])\n",
    "    ###\n",
    "    \n",
    "    ###\n",
    "    print('exporting')\n",
    "    print(train_pair.shape, valid_pair.shape, test_pair_gd.shape)\n",
    "# add the shuffle and split function here\n",
    "    if len(valid_pair)>0:\n",
    "        randomize_split(valid_pair, save_path='cache/valid/')\n",
    "        print('valid')\n",
    "        sys.stdout.flush()\n",
    "    if len(test_pair_gd)>0:\n",
    "        print('test')\n",
    "        sys.stdout.flush()\n",
    "        randomize_split(test_pair_gd, save_path='cache/test/')\n",
    "    if len(train_pair)>0:\n",
    "        randomize_split(train_pair, save_path='cache/train/')\n",
    "        print('train')\n",
    "        sys.stdout.flush()\n",
    "    del valid_pair, train_pair, test_pair_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
